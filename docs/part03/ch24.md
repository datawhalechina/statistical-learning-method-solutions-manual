# 第24章卷积神经网络

## 习题24.1

&emsp;&emsp;设有输入矩阵$X=[x_{ij}]_{I \times J}$，核矩阵$W=[w_{mn}]_{M \times N}$，满足$M \ll I, N \ll J$，则称以下运算为二维数学卷积：
$$
Y = W \circledast X
$$
产生输出矩阵$Y=[y_{kl}]_{K \times L}$，其中
$$
y_{kl} = \sum_{m=1}^M \sum_{n=1}^N w_{mn} x_{i-m+1, j-n+1} \\
K = I - M + 1, L = J - N + 1
$$
证明数学卷积和机器学习卷积有以下关系：
$$
W \circledast X = \text{rot180}(W) * X
$$
这里$\circledast$表示数学卷积，$*$表示机器学习卷积，rot180()表示对矩阵的180度旋转。

**解答：**  

**解答思路：**

1. 给出机器学习二维卷积的定义
2. 计算$\text{rot180}(W) * X$输出矩阵$Y^{\prime}$
3. 验证$Y$与$Y^{\prime}$相等

**解答步骤：**  

**第1步：给出机器学习二维卷积的定义**

&emsp;&emsp;根据书中第417页二维卷积的定义：
> **定义24.1（二维卷积）** 给定一个 $I \times J$ 输入矩阵 $X = [x_{ij}]_{I \times J}$，一个 $M \times N$ 核矩阵 $W=  [w_{mn}]_{M \times N}$，满足 $M \ll I, N \ll J$。让核矩阵在输入矩阵上从左到右再从上到下按顺序滑动，在滑动的每一个位置，核矩阵与输入矩阵的一个子矩阵重叠。求核矩阵与每一个子矩阵的内积，产生一个 $K \times L$ 输出矩阵 $Y= [y_{kl}]_{K \times L}$，称此运算为卷积（convolution）或二维卷积。写作
> $$
Y = W * X \tag{24.4}
$$
> 其中, $Y = [y_{kl}]_{K \times L}$。
> $$
y_{k l}=\sum_{m=1}^M \sum_{n=1}^N w_{m, n} x_{k+m-1, l+n-1}
$$
> 其中，$K=1,2, \cdots, K, l = 1, 2, \cdots, L, K=I-M+1, L=J-N+1$。

**第2步：计算$\text{rot180}(W) * X$输出矩阵$Y^{\prime}$**

&emsp;&emsp;令$W' = \text{rot180}(W) = [w'_{m,n}]_{M \times N} = [w_{M-m+1,N-n+1}]_{M \times N}$，则对于$\text{rot180}(W) * X$输出矩阵$Y' = [y'_{kl}]_{K \times L}$有：

$$
\begin{aligned}
y'_{kl}
&= \sum_{m=1}^M \sum_{n=1}^{N} w'_{m,n} x_{k+m-1, l+n-1} \\
&= \sum_{m=1}^M \sum_{n=1}^{N} w_{M-m+1,N-n+1} x_{k+m-1, l+n-1} \\
\end{aligned}
$$

&emsp;&emsp;令$p=M-m+1$，且当$m = 1$时，$p = M$，当$m = M$时，$p = 1$；令$q=N-n+1$，且当$n = 1$，$q = N$，当$n = N$，$q = 1$，则：

$$
\begin{aligned}
y'_{kl} 
&= \sum_{p=M}^1 \sum_{q=N}^1 w_{p,q} x_{k+M-p,l+N-q} \\
&= \sum_{p=1}^M \sum_{q=1}^N w_{p,q} x_{k+M-p,l+N-q}
\end{aligned}
$$

**第3步：验证$Y$与$Y^{\prime}$相等**

&emsp;&emsp;对于$Y = W \circledast X=[y_{kl}]_{K \times L}$，由题目可知：
$$
y_{kl} = \sum_{m=1}^M \sum_{n=1}^N w_{m, n} x_{i-m+1, j-n+1} \\
K = I - M + 1, L = J - N + 1
$$

可令$i = k + M - 1,j = l + N - 1$，则
$$
i - m + 1 = k + M - m \\
j - n + 1 = l + N -n
$$

可得：
$$
y_{kl} = \sum_{m=1}^M \sum_{n=1}^N w_{mn} x_{k + M-m, l + N-n}
$$

&emsp;&emsp;因此，$W \circledast X = Y = Y^{\prime} = \text{rot180}(W) * X$，故原命题得证。

## 习题24.2

&emsp;&emsp;假设有矩阵
$$
A = \left[ 
\begin{array}{c}
3 & 2 & 0 & 1 \\
0 & 2 & 1 & 2 \\
2 & 0 & 0 & 3 \\
2 & 3 & 1 & 2
\end{array}
\right], B = \left[ 
\begin{array}{c}
2 & 1 & 2 \\
0 & 0 & 3 \\
0 & 0 & 2
\end{array}
\right]
$$
其中，$A$是输入矩阵，$B$是核矩阵，可以求得数学卷积$B \circledast A$是
$$
B \circledast A = \left[ 
\begin{array}{c}
6 & 7 & 8 & 6 & 1 & 3 \\
0 & 4 & 13 & 15 & 4 & 7 \\
4 & 2 & 10 & 16 & 6 & 14 \\
4 & 8 & 15 & 15 & 6 & 17 \\
0 & 0 & 10 & 9 & 3 & 12\\
0 & 0 & 4 & 6 & 2 & 4
\end{array}
\right]
$$
试求数学卷积$A \circledast B$，并验证数学卷积满足交换律。

**解答：**

**解答思路：**  

1. 给出数学卷积的定义
2. 给出二维数学卷积的定义
3. 计算数学卷积$A \circledast B$
4. 验证数学卷积满足交换律

**解答步骤：**   

**第1步：数学卷积的定义**

根据书中第416页数学卷积的定义：
> &emsp;&emsp;在数学中，卷积（convolution）是定义在两个函数上的运算，表示用其中一个函数对另一个函数的形状进行的调整。这里考虑一维卷积。设 $f$ 和 $g$ 是两个可积的实值函数，则积分
> $$
\int_{-\infty}^{+\infty} f(\tau) g(t-\tau) \text{d} \tau
$$
> &emsp;&emsp;定义了一个新的函数 $h (t)$，称为 $f$ 和 $g$ 的卷积，记作
> $$
h(t) = (f \circledast g)(t) = \int_{-\infty}^{+\infty} f(\tau) g(t-\tau) \text{d} \tau
$$
> 其中，符号 $\circledast$ 表示卷积运算。

**第2步：二维数学卷积的定义**

&emsp;&emsp;见习题24.1中二维数学卷积：
$$
Y = W \circledast X
$$
其中输入矩阵$X=[x_{ij}]_{I \times J}$，核矩阵$W=[w_{mn}]_{M \times N}$，满足$M \ll I, N \ll J$，产生输出矩阵$Y=[y_{kl}]_{K \times L}$，其中
$$
y_{kl} = \sum_{m=1}^M \sum_{n=1}^N w_{mn} x_{i-m+1, j-n+1} \\
K = I - M + 1, L = J - N + 1
$$

**第3步：计算数学卷积$A \circledast B$**

根据定义，计算$A \circledast B$

$$
A \circledast B = \left[ 
\begin{array}{c}
6 & 7 & 8 & 6 & 1 & 3 \\
0 & 4 & 13 & 15 & 4 & 7 \\
4 & 2 & 10 & 16 & 6 & 14 \\
4 & 8 & 15 & 15 & 6 & 17 \\
0 & 0 & 10 & 9 & 3 & 12\\
0 & 0 & 4 & 6 & 2 & 4
\end{array}
\right]
$$

**第4步：验证数学卷积满足交换律**

&emsp;&emsp;不妨设
$$
h'(t)=(g \circledast f)(t)=\int_{-\infty}^{+\infty} g(\tau) f(t-\tau) \mathrm{d} \tau
$$

&emsp;&emsp;令 $t- \tau = \lambda$，$\mathrm{d} \lambda = -\mathrm{d}\tau$，且当$\tau \rightarrow +\infty$时，$\lambda \rightarrow -\infty$；当$\tau \rightarrow -\infty$时，$\lambda \rightarrow +\infty$，则：
$$
\begin{aligned}
h'(t)
&= - \int_{+\infty}^{-\infty} g(t - \lambda) f(\lambda) \mathrm{d} \lambda \\
&= \int_{-\infty}^{+\infty} g(t - \lambda) f(\lambda) \mathrm{d} \lambda
\end{aligned}
$$

&emsp;&emsp;因此，$(g \circledast f)(t) = (f \circledast g)(t)$，即数学卷积满足交换律，原命题得证。

## 习题24.3

&emsp;&emsp;验证机器学习卷积（互相关）不满足交换律。

**解答：**

**解答思路：**  

1. 给出机器学习卷积定义
2. 计算$X * W$
3. 验证$X * W$与$W * X$不相等

**解答步骤：**   

**第1步：机器学习卷积定义**

根据书中第441页机器学习卷积的定义：
> 给定输入矩阵$X = [x_{ij}]_{I \times J}$，核矩阵$W = [w_{mn}]_{M \times N}$。让核矩阵在输入矩阵上按顺序滑动，（二维）卷积是定义在核矩阵与输入矩阵的子矩阵的内积，产生输出矩阵$Y=[y_{kl}]_{K \times L}$。
$$
Y = W * X
$$
其中，$Y = [y_{kl}]_{K \times L}$，$\displaystyle y_{kl} = \sum_{m = 1}^M \sum_{n = 1}^N w_{m,n} x_{k + m - 1, l + n - 1}$。

**第2步：计算$X * W$**

&emsp;&emsp;设$X * W$的输出矩阵为$Y' = [y'_{kl}]_{K' \times L'}$，则
$$
y'_{kl} = \sum_{i=1}^{I} \sum_{j=1}^{J} x_{i,j} w_{k+i-1, l+j-1}
$$
其中$K' = 1, 2, \cdots, K', l = 1, 2, \cdots, L', K'= M - I + 1, L' = N - J + 1$。

&emsp;&emsp;令$p = k + i - 1$，当$i = 1$时，$p = k$，当$i = I$时，$p = k + I - 1$；令$q = l + j - 1$，当$j = 1$，$q = l$，当$j = J$，$q = l + J - 1$，则
$$
y'_{kl} = \sum_{p=k}^{p+I-1} \sum_{q=l}^{q+J-1} w_{p, q} x_{p-k+1, q-l+1}
$$

**第3步：验证$X * W$与$W * X$不相等**

&emsp;&emsp;综上，$Y^{\prime} = [y'_{kl}]_{K' \times L'} \neq Y =  [y_{kl}]_{K \times L}$，即 $X * W \neq W * X$，原命题得证。

## 习题24.4

&emsp;&emsp;CNN也可以用于一维数据的处理，求图24.22所示的一维卷积。

![](images/24-4.png)

**解答：**

**解答思路：**  

1. 根据二维卷积的定义，写出一维卷积的定义
2. 计算一维卷积的输出矩阵

**解答步骤：**   

**第1步：根据二维卷积的定义，写出一维卷积的定义**

&emsp;&emsp;给定一个$I \times 1$输入矩阵$X$和$M \times 1$核矩阵$W$，可将它们视作第2维度为1的二维矩阵，则产生的输出矩阵为
$$
Y = W * X = [y_{k,1}]_{K \times 1}
$$
其中
$$
y_{k,1} = \sum_{m=1}^M w_{m, 1} x_{k + m - 1, 1}
$$
其中，$K=1,2, \cdots, K, K = I - M + 1$。

**第2步：计算一维卷积的输出矩阵**

根据上述公式计算可得：

$$
\begin{aligned}
y_{1,1} &= \sum_{m=1}^3 w_{m, 1} x_{1+m-1, 1} = 3 \times 3 + 2 \times 2 = 13 \\
y_{2,1} &= \sum_{m=1}^3 w_{m, 1} x_{2+m-1, 1} = -2 \times 2 = -4 \\
y_{3,1} &= \sum_{m=1}^3 w_{m, 1} x_{3+m-1, 1} = 3 \times 2 + 2 \times 2 = 10 \\
y_{4,1} &= \sum_{m=1}^3 w_{m, 1} x_{4+m-1, 1} = -2 \times 3 + 1 \times 2 = -4 \\
y_{5,1} &= \sum_{m=1}^3 w_{m, 1} x_{5+m-1, 1} = 3 \times 2 + 2 \times 2 = 10
\end{aligned}
$$

一维卷积的输出矩阵为
$$
Y = \left[
\begin{array}{c}
13 \\
-4 \\
10 \\
-4 \\
10 
\end{array}
\right]
$$

## 习题24.5

&emsp;&emsp;通过例24.2验证卷积运算不具有旋转可变性。假设对图像数据进行90度顺时针和逆时针旋转。

**解答：**

**解答思路：**  

1. 将输入矩阵$X$顺时针旋转90度，计算卷积核为$W$时的输出矩阵
2. 将输入矩阵$X$逆时针旋转90度，计算卷积核为$W$时的输出矩阵
3. 验证旋转之后产生的输出矩阵与未旋转产生的输出矩阵不相等

**解答步骤：**   

&emsp;&emsp;例24.2给定的输入矩阵$X$和核矩阵$W$：

$$
X = \left[ 
\begin{array}{c}
3 & 2 & 0 & 1 \\
0 & 2 & 1 & 2 \\
2 & 0 & 0 & 3 \\
2 & 3 & 1 & 2
\end{array}
\right], W = \left[ 
\begin{array}{c}
2 & 1 & 2 \\
0 & 0 & 3 \\
0 & 0 & 2
\end{array}
\right]
$$

**第1步：将输入矩阵$X$顺时针旋转90度，计算卷积核为$W$时的输出矩阵**

&emsp;&emsp;将输入矩阵$X$顺时针旋转90度，得：

$$
X_{rot90} = \text{rot90}(X) = \left[ 
\begin{array}{cccc}
2 & 2 & 0 & 3 \\
3 & 0 & 2 & 2 \\
1 & 0 & 1 & 0 \\
2 & 3 & 2 & 1
\end{array}
\right]
$$

&emsp;&emsp;根据二维卷积定义，得到输出矩阵

$$
Y_{rot90} = W * X_{rot90} = \left[ 
\begin{array}{ccc}
2 & 1 & 2 \\
0 & 0 & 3 \\
0 & 0 & 2
\end{array}
\right] * \left[ 
\begin{array}{cccc}
2 & 2 & 0 & 3 \\
3 & 0 & 2 & 2 \\
1 & 0 & 1 & 0 \\
2 & 3 & 2 & 1
\end{array}
\right] = \left[ 
\begin{array}{cc}
14 & 16 \\
17 & 18 
\end{array}
\right]
$$

**第2步：将输入矩阵$X$逆时针旋转90度，计算卷积核为$W$时的输出矩阵**

&emsp;&emsp;将输入矩阵$X$逆时针旋转90度，得：

$$
X_{rot-90} = \text{rot-90}(X) = \left[ 
\begin{array}{cccc}
1 & 2 & 3 & 2 \\
0 & 1 & 0 & 1 \\
2 & 2 & 0 & 3 \\
3 & 0 & 2 & 2
\end{array}
\right]
$$

&emsp;&emsp;根据二维卷积定义，得到输出矩阵：

$$
Y_{rot-90} = W * X_{rot-90} = \left[ 
\begin{array}{ccc}
2 & 1 & 2 \\
0 & 0 & 3 \\
0 & 0 & 2
\end{array}
\right] * \left[ 
\begin{array}{ccccc}
1 & 2 & 3 & 2 \\
0 & 1 & 0 & 1 \\
2 & 2 & 0 & 3 \\
3 & 0 & 2 & 2
\end{array}
\right] = \left[ 
\begin{array}{cc}
10 & 20 \\
5 & 17 
\end{array}
\right]
$$

**第3步：验证旋转之后产生的输出矩阵与未旋转产生的输出矩阵不相等**

&emsp;&emsp;由例24.2可知，
$$
Y = W * X = \left[ 
\begin{array}{ccc}
2 & 1 & 2 \\
0 & 0 & 3 \\
0 & 0 & 2
\end{array}
\right] * \left[ 
\begin{array}{cccc}
3 & 2 & 0 & 1 \\
0 & 2 & 1 & 2 \\
2 & 0 & 0 & 3 \\
2 & 3 & 1 & 2
\end{array}
\right] = \left[ 
\begin{array}{cc}
11 & 18 \\
6  & 22 
\end{array}
\right]
$$

&emsp;&emsp;显然，$Y \neq Y_{rot90} \neq Y_{rot-90}$，即卷积运算不具有旋转可变性。

## 习题24.6

&emsp;&emsp;证明感受野的关系式（24.19）成立。

**解答：**

**解答思路：**  

1. 给出感受野的关系式
2. 写出感受野递推形式的计算公式
3. 用数学归纳法证明感受野关系式成立

**解答步骤：**   

**第1步：感受野的关系式**

根据书中第432页感受野的关系式：
> &emsp;&emsp;考虑卷积神经网络全部由卷积层组成的情况，神经元的感受野的大小有以下关系成立。
> $$
R^{(l)} = 1 + \sum_{j=1}^l \left( F^{(j)} - 1 \right) \prod_{i=0}^{j - 1} S^{(i)} \tag{24.19}
$$
> 设输入矩阵和卷移核都呈正方形。$R^{(l)} \times R^{(l)}$ 表示第 $l$ 层的神经元的感受野的大小，$F^{(j)} \times F^{(j)}$ 表示第 $j$ 层的卷积核的大小，$S^{(i)}$ 表示第 $i$ 层卷积的步幅，设 $S^{(0)} =1$。

**第2步：写出感受野递推形式的计算公式**

&emsp;&emsp;每个卷积层的每个神经元只与上一层的一部分神经元相连，故我们可根据上一层卷积层情况推导出下一层的感受野，感受野递推形式的计算公式如下：
$$
R^{(l)}=R^{(l-1)} + (F^{(l)} - 1) I^{(l)}
$$
其中，$R^{(l)} \times R^{(l)}$和$R^{(l-1)} \times R^{(l-1)}$分别表示第$l$和$l-1$层的神经元的感受野的大小，$F^{(j)} \times F^{(j)}$ 表示第 $j$ 层的卷积核的大小。$I^{(l)}$表示第$l$层输出特征图的特征间的间隔，由$I^{(l)} = I^{(l-1)} \cdot S^{l-1}$，设$I^{(0)}=1$。

**第3步：用数学归纳法证明感受野关系式成立**

&emsp;&emsp;当$l = 1$时，$R^{1} = 1 + (F^1 - 1) = F^1$，显然成立

&emsp;&emsp;假设当 $l = n$ 时，
$$
R^{(n)} = 1 + \sum_{j=1}^n \left( F^{(j)} - 1 \right) \prod_{i=0}^{j-1} S^{(i)}
$$
&emsp;&emsp;成立。

&emsp;&emsp;当$l = n+1$时，
$$
\begin{aligned}
R^{(n+1)}
&= R^{(n)} + \left(F^{(n+1)}-1\right) I^{n+1} \\
&= R^{(n)} + \left(F^{(n+1)}-1\right) I^{n}  S^{n} \\
&= R^{(n)} + \left(F^{(n+1)}-1\right) I^{n-1}   S^{n-1} S^{n} \\
&= R^{(n)} + \left(F^{(n+1)}-1\right) I^{0} S^{0}  S^{1} \cdots  S^{n-1}  S^{n} \\
&= R^{(n)} + \left(F^{(n+1)}-1\right) \prod_{i=0}^{n} S^{(i)} \\
&= 1 + \sum_{j=1}^n \left( F^{(j)} - 1 \right) \prod_{i=0}^{j-1} S^{(i)} + \left( F^{(n+1)} - 1 \right) \prod_{i=0}^{n} S^{(i)} \\
&= 1 + \sum_{j=1}^{n+1} \left( F^{(j)} - 1 \right) \prod_{i=0}^{j-1} S^{(i)}
\end{aligned}
$$

结合$R^{(n)}$和$R^{(n+1)}$的关系式，可得到：
$$
R^{(l)} = 1 + \sum_{j=1}^l \left( F^{(j)} - 1 \right) \prod_{i=0}^{j - 1} S^{(i)}
$$

## 习题24.7

&emsp;&emsp;设计一个基于CNN的自然语言句子分类模型。假设句子是单词序列，每个单词用一个实数向量表示。

**解答：**

**解答思路：**  

1. 给出自然语言句子分类模型的参考文献和相关的模型架构图
2. 使用PyTorch自编程实现模型

**解答步骤：**   

**第1步：给出自然语言句子分类模型的参考文献和相关的模型架构图**

根据参考文献[Chen, Yahui. Convolutional neural network for sentence classification. MS thesis. University of Waterloo, 2015.](https://arxiv.org/abs/1408.5882)提出了一种基于CNN的自然语言句子分类模型，其模型架构图如下

![](images/24-7.png)

**第2步：使用PyTorch自编程实现模型**


```python
import torch
from torch import nn
from torch import functional as F


class CnnTextClassifier(nn.Module):

    def __init__(self, 
                 max_token_num, 
                 embedding_dim, 
                 output_dim, 
                 kernel_num, 
                 kernel_sizes=[2, 3, 4], 
                 dropout_rate=0.5):
        """
        基于CNN的自然语言句子分类模型

        :param max_token_num: 单词表最大数量
        :param embedding_dim: 词嵌入维度
        :param output_dim: 句子类别个数
        :param kernel_num: 每层卷积的核数量
        :param kernel_sizes: 每层卷积的核大小
        :param dropout_rate: dropout概率
        """
        self.emb = nn.Embedding(max_token_num, embedding_dim)
        self.convs = 1
        layers_list = []
        for i, kernel_size in enumerate(kernel_sizes):
            layers_list.append(nn.Conv2d(1, kernel_num, kernel_size=(kernel_size, embedding_dim), stride=1))
        self.convs = nn.ModuleList(layers_list)
        self.dropout = nn.Dropout(dropout_rate)
        self.lin = nn.Linear(len(kernel_sizes) * kernel_num, output_dim)

    def forward(self, x):
        """
        前向传播，计算句子类别概率分布

        :param x: 批量的句子，形状为(batch_size, max_seq_length)
        """
        # 输出词嵌入结果，形状为(batch_size, max_seq_length, embedding_dim)
        x = self.emb(x) 
        # 扩展维度为(batch_size, 1, max_seq_length, embedding_dim)，使2D卷积可操作    
        x = x.unsqueeze(1)  
        # 卷积操作，输出形状为[(batch_size, kernel_num, max_seq_length), ...]*len(kernel_sizes)
        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs]  
        # 池化操作，输出形状为[(batch_size, kernel_num), ...]*len(kernel_sizes)
        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]   
        # 拼接操作，输出形状为(batch_size, len(kernel_sizes)*kernel_num)
        x = torch.cat(x, 1)
        x = self.dropout(x)  
        # 计算分类概率，输出形状为(batch_size, output_dim)
        logit = self.lin(x) 
        return logit
```

## 习题24.8

&emsp;&emsp;设有输入矩阵$X$和核矩阵$W$：
$$
X = [x_{ij}] = \left[ 
\begin{array}{c}
x_{11} & x_{12} & x_{13} & x_{14} & x_{15} \\
x_{21} & x_{22} & x_{23} & x_{24} & x_{25} \\
x_{31} & x_{32} & x_{33} & x_{34} & x_{35} \\
x_{41} & x_{42} & x_{43} & x_{44} & x_{45} 
\end{array}
\right], W = [w_{mn}] = \left[ 
\begin{array}{c}
w_{11} & w_{12} & w_{13} \\
w_{21} & w_{22} & w_{23} \\
w_{31} & w_{32} & w_{33} 
\end{array}
\right]
$$
有卷积$Y = W * X$：
$$
Y = [y_{kl}] = \left[ 
\begin{array}{c}
y_{11} & y_{12} & y_{13} \\
y_{21} & y_{22} & y_{23} \\
y_{31} & y_{32} & y_{33} 
\end{array}
\right]
$$
求$\displaystyle \frac{\partial Y}{\partial W}$和$\displaystyle \frac{\partial Y}{\partial X}$，并具体地写出$\displaystyle \frac{\text{d} y_{kl}}{\text{d} w_{mn}}$和$\displaystyle \frac{\text{d} y_{kl}}{\text{d} x_{ij}}$。  

**解答：**

**解答思路：**  

1. 给出二维卷积定义
2. 计算 $\displaystyle \frac{\partial Y}{\partial W}$和$\displaystyle \frac{\partial Y}{\partial X}$
3. 计算 $\displaystyle \frac{\text{d} y_{kl}}{ \text{d} w_{mn}}$和$\displaystyle \frac{\text{d} y_{kl}}{\text{d} x_{ij}}$

**解答步骤：**   

**第1步：二维卷积定义**

根据书中第441页二维卷积的定义：
> 给定输入矩阵$X = [x_{ij}]_{I \times J}$，核矩阵$W = [w_{mn}]_{M \times N}$。让核矩阵在输入矩阵上按顺序滑动，（二维）卷积是定义在核矩阵与输入矩阵的子矩阵的内积，产生输出矩阵$Y=[y_{kl}]_{K \times L}$。
$$
Y = W * X
$$
其中，$Y = [y_{kl}]_{K \times L}$。
$$
y_{kl} = \sum_{m = 1}^M \sum_{n = 1}^N w_{m,n} x_{k + m - 1, l + n - 1}
$$
其中，$k = 1, 2, \cdots, K, l = 1,2,\cdots, L, K = I - M + 1, L = J - N + 1$。

**第2步：计算 $\displaystyle \frac{\partial Y}{\partial W}$和$\displaystyle \frac{\partial Y}{\partial X}$**

1. 计算 $\displaystyle \frac{\partial Y}{\partial W}$

$$
\begin{aligned}
\frac{\partial Y}{\partial W} 
&= \frac{\partial Y}{\partial w_{m,n}} \\
&= \sum_{k=1}^K \sum_{l=1}^L \frac{\partial y_{kl}}{\partial w_{m,n}} \\
&= \sum_{k=1}^K \sum_{l=1}^L \frac{\partial (w_{m,n} x_{k+m-1, l+n-1})}{\partial w_{m,n}} \\
&= \sum_{k=1}^K \sum_{l=1}^L x_{k+m-1, l+n-1}
\end{aligned}
$$

2. 计算 $\displaystyle \frac{\partial Y}{\partial X}$

根据二维卷积的定义，可得：
$$
m = i - k + 1 \\
n = j - l + 1 
$$

则：
$$
\begin{aligned}
\frac{\partial Y}{\partial X} 
&= \frac{\partial Y}{\partial x_{ij}} \\
&= \sum_{k=1}^K \sum_{l=1}^L \frac{\partial y_{kl}}{\partial x_{i j}} \\
&= \sum_{k=1}^K \sum_{l=1}^L \frac{\partial (w_{i - k + 1, j - l + 1} x_{i j})}{\partial x_{i j}} \\
&= \sum_{k=1}^K \sum_{l=1}^L w_{i - k + 1, j - l + 1}
\end{aligned}
$$

**第2步：计算 $\displaystyle \frac{\text{d} y_{kl}}{\text{d} w_{mn}}$和$\displaystyle \frac{\text{d} y_{kl}}{\text{d} x_{ij}}$**

1. 计算 $\displaystyle \frac{\text{d} y_{kl}}{\text{d} w_{mn}}$

$$
\begin{aligned}
\frac{\text{d} y_{kl}}{\text{d} w_{mn}}
&= \frac{\displaystyle \text{d} \left( \sum_{m=1}^M \sum_{n=1}^N {w_{m,n} x_{k+m-1, l+n-1}} \right)}{dw_{mn}} \\
&= x_{k+m-1, l+n-1}
\end{aligned}
$$

2. 计算 $\displaystyle \frac{\text{d} y_{kl}}{\text{d} x_{ij}}$

&emsp;&emsp;首先计算 $\displaystyle \frac{\text{d} y_{kl}}{\text{d} x_{k+m-1, l+n-1}}$
$$
\begin{aligned}
\frac{\text{d} y_{kl}}{\text{d} x_{k+m-1, l+n-1}}
&= \frac{\displaystyle \text{d} \left( \sum_{m=1}^M \sum_{n=1}^N {w_{m,n} x_{k+m-1, l+n-1}} \right)}{\text{d} x_{k+m-1, l+n-1}} \\
&= w_{mn}
\end{aligned}
$$

&emsp;&emsp;令$i = k + m - 1$，且当$m=1$时，$i = k$，当$m = M$时，$i = k + M -1$；令$j = l + n - 1$，且当$n=1$时，$j = l$，当$n = N$时，$j = l + N - 1$。

&emsp;&emsp;则当 $i \in [k, k + M -1]$ 且 $j \in [l, l + N -1]$时，可得：
$$
\frac{\text{d} y_{kl}}{\text{d} x_{ij}} = w_{mn} = w_{i - k + 1, i-l+1}
$$

&emsp;&emsp;当 $i \notin [k, k + M -1]$ 或 $j \notin [l, l + N -1]$时，可得：
$$
\frac{\text{d} y_{kl}}{\text{d} x_{ij}} = 0
$$

&emsp;&emsp;综上，

$$
\frac{\text{d} y_{kl}}{\text{d} x_{ij}}
= 
\begin{cases}
    w_{i - k + 1, i - l + 1}, & \text{if } i \in [k, k + M - 1] 且 j \in [l, l + N - 1] \\
    0,              & \text{otherwise}
\end{cases}
$$

## 习题24.9
&emsp;&emsp;设有输入矩阵$X$和核矩阵$W$：
$$
X = \left[ 
\begin{array}{c}
x_{11} & x_{12} & x_{13} \\
x_{21} & x_{22} & x_{23} \\
x_{31} & x_{32} & x_{33} 
\end{array}
\right], W = \left[ 
\begin{array}{c}
w_{11} & w_{12} & w_{13} \\
w_{21} & w_{22} & w_{23} \\
w_{31} & w_{32} & w_{33} 
\end{array}
\right]
$$
验证$\text{rot180}(W) * X = \text{rot180}(X) * W$成立。

**解答：**

**解答思路：**  

1. 给出机器学习卷积的定义
2. 计算$\text{rot180}(W) * X$
3. 计算$\text{rot180}(X) * W$
4. 验证$\text{rot180}(W) * X$与$\text{rot180}(X) * W$是否相等

**解答步骤：**   

**第1步：给出机器学习卷积的定义**

根据书中第441页二维卷积的定义：
> 给定输入矩阵$X = [x_{ij}]_{I \times J}$，核矩阵$W = [w_{mn}]_{M \times N}$。让核矩阵在输入矩阵上按顺序滑动，（二维）卷积是定义在核矩阵与输入矩阵的子矩阵的内积，产生输出矩阵$Y=[y_{kl}]_{K \times L}$。
$$
Y = W * X
$$
其中，$Y = [y_{kl}]_{K \times L}$。
$$
y_{kl} = \sum_{m = 1}^M \sum_{n = 1}^N w_{m,n} x_{k + m - 1, l + n - 1}
$$
其中，$k = 1, 2, \cdots, K, l = 1,2,\cdots, L, K = I - M + 1, L = J - N + 1$。

**第2步：计算$\text{rot180}(W) * X$**

&emsp;&emsp;由题意已知：
$$
\text{rot180}(W) = [w'_{m,n}] = [w_{4-m, 4-n}]
$$

&emsp;&emsp;由机器学习卷积定义，可得：
$$
\begin{aligned}
Y'
&= \text{rot180}(W) * X \\
&= [y'_{kl}] \\
&= \sum_{m=1}^{3} \sum_{n=1}^{3} w'_{m, n} x_{k+m-1, l+n-1} \\
&= \sum_{m=1}^{3} \sum_{n=1}^{3} w_{4-m, 4-n} x_{k+m-1, l+n-1}
\end{aligned}
$$

**第3步：计算$\text{rot180}(X) * W$**

&emsp;&emsp;由题意已知：
$$
\text{rot180}(X) = [x'_{i,j}] = [x_{4-i, 4-j}]
$$

&emsp;&emsp;由机器学习卷积定义，可得：
$$
\begin{aligned}
Y''
&= \text{rot180}(X) * W \\
&= [y''_{kl}] \\
&= \sum_{i=1}^{3} \sum_{j=1}^{3} x'_{i, j} w_{k+i-1, l+j-1} \\
&= \sum_{i=1}^{3} \sum_{j=1}^{3} x_{4-i, 4-j} w_{k+i-1, l+j-1}
\end{aligned}
$$

**第4步：验证$\text{rot180}(W) * X$与$\text{rot180}(X) * W$是否相等**

&emsp;&emsp;由上述计算可知：

$$
Y' = \sum_{m=1}^{3} \sum_{n=1}^{3} w_{4-m, 4-n} x_{k+m-1, l+n-1} \\
Y'' = \sum_{i=1}^{3} \sum_{j=1}^{3} x_{4-i, 4-j} w_{k+i-1, l+j-1} 
$$

&emsp;&emsp;令 $4 - q = k + i - 1$，且当$i = 1$时，$q = 4 - k$，当$i = 3$时，$q  = 2 - k$；

&emsp;&emsp;又令 $4 - p = l + j - 1$，且当$j = 1$时，$p = 4 - k$，当$j = 3$时，$p  = 2 - k$，  

&emsp;&emsp;则 
$$
i = 5 - q - k \\ 
j = 5 - p - l
$$

&emsp;&emsp;因此
$$
\begin{aligned}
Y'' 
&= [y''_{k,l}] \\
&= \sum_{i=1}^{3} \sum_{j=1}^{3} x_{4-i, 4-j} w_{k+i-1, l+j-1} \\
&= \sum_{q = 4 - k}^{2 - k} \sum_{p = 4 - k}^{2 - k} w_{4-q, 4-p} x_{k + q - 1, k + p - 1}
\end{aligned}
$$

&emsp;&emsp;综上，$Y' = Y''$，即 $\text{rot180}(W) * X = \text{rot180}(X) * W $，原命题得证。

## 习题24.10
&emsp;&emsp;解释残差网络为什么能防止梯度消失和梯度爆炸。

**解答：**

**解答思路：**  

1. 给出残差网络的基本概念
1. 给出梯度消失和梯度爆炸的现象
1. 解释残差网络能防止梯度消失和梯度爆炸的原因

**解答步骤：**   

**第1步：残差网络的基本概念**

&emsp;&emsp;根据书中第443页残差网络的描述：

> &emsp;&emsp;残差网络是为了解决深度神经网络训练困难而提出的深度学习方法。假设要学习的真实模型是函数 $h (x)$，也可以写作
>$$
h(x) = x + (h(x) - x)
$$
> &emsp;&emsp;残差网络的想法是用一个神经网络 $f(x)$ 近似残差 $h(x) - x$，用 $x + f(x)$ 近似真实模型 $h(x)$，整个过程以递归的方式进行。
> $$
x_i=x_i-1+f_i (x_i-1), i=1,2, \cdots, n
$$
> &emsp;&emsp;残差网络由很多个残差单元串联连接组成。每一个残差单元相当于一般的前馈网络的两层，每一层由线性变换和非线性变换组成，还有一个残差连接。  
> &emsp;&emsp;残差单元的输入是向量 $x$，输出是向量 $y$ 时，整个单元的运算是
> $$
f(x) = W_2 \text{relu} (W_1 x) \\
y = \text{relu} (x + f(x))
$$
> &emsp;&emsp;残差网络可以展开成多个神经网络模块的集成，有很强的表示和学习能力。

**第2步：梯度消失和梯度爆炸的现象**

根据书中第401页\~第402页梯度消失与梯度爆炸：
> &emsp;&emsp;深度神经网络学习中有时会出现梯度消失(vanishing gradient)或者梯度爆炸(exploding gradient)现象。使用反向传播算法时，首先通过正向传播计算神经网络各层的输出，然后通过反向传播神经网络各层的误差以及梯度，接着利用梯度下降公式对神经网络各层的参数进行更新。在这个过程中，各层的梯度，特别是前面层的梯度，有时会接近0（梯度消失）或接近无穷（梯度爆炸）。梯度消失会导致参数更新停止，梯度爆炸会导致参数溢出，都会使学习无法有效地进行。
> 反向传播中，首先计算误差向量：
> $$
\delta^{(t - 1)} = \left \{ \text{diag}\left( \frac{\partial a}{\partial z^{(t - 1)}} \right) \cdot {W^{(t)}}^T \right \} \cdot \delta^{(t)} \tag{23.57}
$$
> 之后计算梯度：
> $$
\left\{ 
\begin{array}{l}
\nabla_{W^{(t)}} L = \delta^{(t)} \cdot {h^{(t - 1)}}^T \\
\nabla_{b^{(t)}} L = \delta^{(t)}
\end{array}
\right . \tag{23.58}
$$
> 造成梯度消失和梯度爆炸的原因有两种：
> 1. 每一层的误差向量实际由矩阵的连乘决定，连乘得到的矩阵的元素可能会接近0，也可能会接近无穷，导致梯度的元素也会接近0或接近无穷，而且越是前面的层，这个问题就越严重。
> 2. 得到每一层的误差向量之前，每个元素乘以激活函数的导数，如果激活函数的导数过小，也容易引起梯度消失，而且越是前面的层，这个问题就越严重。

&emsp;&emsp;假设 $L$ 表示神经网络的层数，$w_{i,j}^{(l)}$ 表示第 $l$ 层中第 $i$ 个神经元到第 $l+1$ 层中第 $j$ 个神经元的权重，$a_i^{(l)}$ 表示第 $l$ 层中第 $i$ 个神经元的激活值，$z_i^{(l)}$ 表示第 $l$ 层中第 $i$ 个神经元的加权输入，$g(z)$ 表示激活函数，$y$ 表示网络的输出值，$y_i$ 表示输出向量的第 $i$ 个元素。则网络的输出可以表示为：

$$
y=g\left(z^{(L)}\right)=g\left(\boldsymbol{w}^{(L)} g\left(\boldsymbol{w}^{(L-1)} g\left(\cdots g\left(\boldsymbol{w}^{(1)} \boldsymbol{x}\right)\right)\right)\right)
$$
其中 $\boldsymbol{x}$表示输入变量。

&emsp;&emsp;对于一个损失函数 $J(y, \hat{y})$，网络的参数 $\boldsymbol{w}$ 可以通过梯度下降法来更新：

$$
\boldsymbol{w}_{t+1}=\boldsymbol{w}_t-\eta \frac{\partial J(y, \hat{y})}{\partial \boldsymbol{w}}
$$

&emsp;&emsp;在反向传播过程中，可以通过链式法则来计算每个神经元的梯度。具体地，对于第 $l$ 层中第 $i$ 个神经元的梯度，可以表示为：

$$
\frac{\partial J}{\partial z_i^{(l)}}=\sum_{j=1}^{m^{(l+1)}} \frac{\partial J}{\partial z_j^{(l+1)}} \frac{\partial z_j^{(l+1)}}{\partial z_i^{(l)}}
$$
其中 $m^{(l+1)}$ 表示第 $l+1$ 层的神经元数量。然后，可以使用梯度下降法来更新参数。

&emsp;&emsp;因为梯度是从输出层向输入层反向传播的，则梯度的大小可以表示为：

$$
\left|\frac{\partial J}{\partial z_i^{(l)}}\right|=\left|\sum_{j=1}^{m^{(l+1)}} \frac{\partial J}{\partial z_j^{(l+1)}} \frac{\partial z_j^{(l+1)}}{\partial z_i^{(l)}}\right| .
$$

&emsp;&emsp;由于 $z_j^{(l+1)}$ 依赖于 $z_i^{(l)}$，因此 $\frac{\partial z_j^{(l+1)}}{\partial z_i^{(l)}}$ 可能会接近0或接近无穷，导致梯度爆炸或梯度消失的情况。

&emsp;&emsp;如果 $\displaystyle \left| \frac{\partial z_j^{(l+1)}}{\partial z_i^{(l)}} \right| > 1 $，那么在反向传播时，梯度会呈现指数级增长，导致梯度爆炸的问题。这种情况通常会导致权重的值变得非常大，从而使模型无法收敛。

&emsp;&emsp;相反，如果 $\displaystyle \left|\frac{\partial z_j^{(l+1)}}{\partial z_i^{(l)}}\right| < 1$，那么在反向传播时梯度会逐渐变小，导致梯度消失的问题。这种情况通常会导致网络无法学习到深层特征，从而导致模型性能下降。

**第3步：解释残差网络能防止梯度消失和梯度爆炸的原因**

&emsp;&emsp;在残差网络中，每个残差单元可以表示为 $y = f(x) + x$，其中 $x$ 表示输入，$f(x)$ 表示卷积层的输出，$y$ 表示残差单元的输出。从数学的角度来看，这个残差单元可以被看作是一个恒等映射加上一个残差映射，即 $y=x+\Delta(x)$，其中 $\Delta(x) = f(x)-x$ 表示残差。

&emsp;&emsp;简单地，考虑这个残差单元的导数。计算 $y$ 对 $x$ 的导数有：

$$
\frac{\partial y}{\partial x}=\frac{\partial y}{\partial f(x)} \frac{\partial f(x)}{\partial x}+\frac{\partial y}{\partial x}=\frac{\partial f(x)}{\partial x}+1
$$

&emsp;&emsp;残差网络能防止梯度消失和梯度爆炸的原因如下：

1. 由于残差网络中的 $\displaystyle \frac{\partial f(x)}{\partial x}$不可能一直为-1，因此在残差网络中不容易出现梯度消失的问题。

2. 由于残差网络中的残差映射是通过将输入直接加到输出上实现的，因此它不会影响导数的大小。这意味着如果 $\displaystyle \frac{\partial y}{\partial x}$ 接近于 1，那么 $\displaystyle \frac{\partial J}{\partial x}$（$J$ 表示损失函数）的值也会比较稳定，不会出现梯度消失或梯度爆炸的问题。

3. 由于残差映射中的 $\Delta(x)$ 只包含了局部信息，而没有涉及到全局信息，因此它通常不会影响梯度的大小。这意味着即使 $\displaystyle \frac{\partial y}{\partial x}$ 很小，梯度也可以通过残差映射传递到后面的层中，从而避免了梯度消失的问题。
