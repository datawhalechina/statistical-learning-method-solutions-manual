{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 第25章循环神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 习题25.1\n",
    "&emsp;&emsp;Jordan提出的循环神经网络如图25.15所示。试写出这种神经网络的公式，并与Elman提出的简单循环神经网络做比较。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答思路：**\n",
    "1. 给出Jordan RNN的公式表达\n",
    "2. 从直接内涵和表达差异两个出发点给出比较"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答步骤：**  \n",
    "\n",
    "**第1步：给出Jordan RNN的公式表达**\n",
    "\n",
    "Jordan RNN:\n",
    "\n",
    "defination: \n",
    "[$tanh$](https://wuu.wikipedia.org/wiki/%E5%8F%8C%E6%9B%B2%E5%87%BD%E6%95%B0),\n",
    "[$softmax$](https://zh.wikipedia.org/zh-hans/Softmax%E5%87%BD%E6%95%B0)\n",
    "\n",
    "$$\n",
    "\\begin{array}{c}\n",
    "\\boldsymbol{r}_{t}=\\boldsymbol{U} \\cdot \\boldsymbol{p}_{t-1}+\\boldsymbol{W} \\cdot \\boldsymbol{x}_{t}+\\boldsymbol{b} \\\\\n",
    "\\boldsymbol{h}_{t}=\\tanh \\left(\\boldsymbol{r}_{t}\\right) \\\\\n",
    "\\boldsymbol{z}_{t}=\\boldsymbol{V} \\cdot \\boldsymbol{h}_{t}+\\boldsymbol{c} \\\\\n",
    "\\boldsymbol{p}_{t}=\\operatorname{softmax}\\left(\\boldsymbol{z}_{t}\\right)\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "**第2步：从直接内涵和表达差异两个出发点给出比较**\n",
    "\n",
    "相同点：\n",
    "1. 都是RNN的经典实现，都是基于三层最基本的网络层实现的网络单元，都可以实现时序的序列信息利用\n",
    "\n",
    "差异：\n",
    "1. Jordan RNN 采用softmax处理隐含层后的输出层作为下一层隐含层的输入，而Elman RNN 采用的是softmax处理前的隐含层作为下一层隐含层的输入\n",
    "2. 鉴于第一点，由于Jordan RNN采用的是经softmax处理后的隐含层，所以其分布较原本的隐含层改变了，尤其是类别间的差距被非线性放大或缩小（负值厌恶），这种对正负向的偏好在信息量表达上是不利的，这里直接建模相邻时间节点的隐含层可以建立更直接的相邻隐含层分布之间的关系，拥有更高的拓展性和普适性，所以后续的循环神经网络多以Elman RNN作为基础机构。\n",
    "3. Elman network的每一个循环层都是相互独立的，因此网络结构的设计可以更加灵活。当Jordan network的输出层与循环层的维度不一致时还需要额外的调整，而Elmannetwork则不存在该问题\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 习题25.2\n",
    "&emsp;&emsp;写出循环神经网络的层归一化的公式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：**\n",
    "\n",
    "**解答思路：**  \n",
    "\n",
    "介绍归一化公式，介绍RNN中归一化的使用过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答步骤：**  \n",
    "\n",
    "对于第l层循环神经网络曾输入为$z^{(l)}$，其层归一化后的输出$\\tilde{z}^{(l)}$为：\n",
    "$$\n",
    "\\tilde{z}^{(l)}=\\frac{z^{(l)}-\\mu^{(l)}}{\\sqrt{\\sigma^{(l)}+\\epsilon}} \\odot \\gamma+\\beta \\Leftarrow L N_{\\gamma, \\beta}\\left(z^{(l)}\\right)\n",
    "$$\n",
    "\n",
    "其中$\\gamma$, $\\beta$ 这里是缩放和平移的参数向量，和 $z^{(l)}$ 的维度相同，$\\mu^{(l)}$为：\n",
    "$$\n",
    "\\mu^{(l)}=\\frac{1}{n^{(l)}} \\sum_{i=1}^{n^{(l)}} z_i^{(l)}\n",
    "$$\n",
    "\n",
    "在循环神经网络中：\n",
    "\n",
    "$$\n",
    "z_t=U h_{t-1}+W x_t, \\quad h_t=f\\left(L N_{\\gamma, \\beta}\\left(z_t\\right)\\right)\n",
    "$$\n",
    "其中$x_t$ 为t时刻的输入 $U$, $W$ 为网络参数\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 习题25.3\n",
    "&emsp;&emsp;比较前馈神经网络的反向传播算法与循环神经网络的反向传播算法的异同。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：**\n",
    "\n",
    "**解答思路：**  \n",
    "1. 前馈神经网络的反向传播算法\n",
    "2. 循环神经网络的反向传播算法\n",
    "3. 比较二者异同"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答步骤：**   \n",
    "\n",
    "**第1步：前馈神经网络的反向传播算法**\n",
    "\n",
    "可根据书中第393-397页查看前馈神经网络的反向传播算法。\n",
    "\n",
    "> 输入：神经网络$f(x;\\theta)$,参数向量$\\theta$，一个样本$(x,y)$\\\n",
    "> 输出: 更新的参数向量$\\theta$\\\n",
    "> 超参数: 学习率 $\\eta$\n",
    "\n",
    "**第2步：循环神经网络的反向传播算法**\n",
    "\n",
    "可根据书中第450-454页查看循环神经网络的反向传播算法。\n",
    "\n",
    "> 输入：循环神经网络$f(x;\\theta)$,参数$\\theta$，样本$(x_1,x_2,...,x_T)$ 和$(y_1,y_2,...,y_T)$\\\n",
    "> 输出: 更新的参数$\\theta$\\\n",
    "> 超参数: 学习率 $\\eta$\n",
    "\n",
    "**第3步：比较异同**\n",
    "\n",
    "两种神经网络的反向传播学习过程都包含四个标准步骤:\n",
    "1. 正向传播,得到各个位置的输出\n",
    "2. 反向传播,得到各个位置的梯度\n",
    "3. 进行参数更新\n",
    "4. 返回更新的参数\n",
    "这两种网络的反向传播过程都会会产生梯度消失和梯度爆炸,尤其是前面层的梯度,梯度消失会造成参数更新停止,而梯度爆炸则会造成单数溢出,都会使得学习无法有效进行. \n",
    "\n",
    "梯度消失和梯度爆炸产生的原因有两种:\n",
    "1. 每一层的误差向量实际由矩阵的连乘决定,连乘得到的矩阵的元素可能会接近0,也可能会接近无穷,导致梯度的元素也会接近0或接近无穷,而且越是前面的层这个问题就越严重.\n",
    "2. 得到的每一层的误差向量之前的每个元素乘以激活函数的倒数,如果激活函数的导数过小,也容易引起梯度消失,而且越是前面的层,这个问题就会越严重.\n",
    "\n",
    "在循环网络的反向传播中,第一点原因导致的梯度消失与爆炸的风险更严重了,因为矩阵的连乘接近矩阵的连续自乘,而前馈神经网络一般不是."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 习题25.4\n",
    "&emsp;&emsp;写出LSTM模型的反向传播算法公式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：**\n",
    "\n",
    "**解答思路：**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答步骤：**   \n",
    "\n",
    "**第一步: 在t时刻的前向传播公式为：**\n",
    "\n",
    "$\\left\\{\\begin{array}{l}\n",
    "i_t=\\sigma\\left(\\tilde{i}_t\\right)=\\sigma\\left(W_{x i} x_t+W_{h i} h_{t-1}+W_{c i} c_{t-1}+b_i\\right) \\\\\n",
    "f_t=\\sigma\\left(\\tilde{f}_t\\right)=\\sigma\\left(W_{x f} x_t+W_{h f} h_{t-1}+W_{c f} c_{t-1}+b_f\\right) \\\\\n",
    "g_t=\\tanh \\left(\\tilde{g}_t\\right)=\\tanh \\left(W_{x g} x_t+W_{h g} h_{t-1}+b_g\\right) \\\\\n",
    "o_t=\\sigma\\left(\\tilde{o}_t\\right)=\\sigma\\left(W_{x o} x_t+W_{h o} h_{t-1}+W_{c o} c_t+b_o\\right) \\\\\n",
    "c_t=c_{t-1} \\odot f_t+g_t \\odot i_t \\\\\n",
    "m_t=\\tanh \\left(c_t\\right) \\\\\n",
    "h_t=o_t \\odot m_t \\\\\n",
    "y_t=W_{y h} h_t+b_y\n",
    "\\end{array}\\right.$\n",
    "\n",
    "**第二步: 反向传播：**\n",
    "已知: $\\frac{\\partial J}{\\partial y_t}, \\frac{\\partial J}{\\partial c_{t+1}}, \\frac{\\partial J}{\\partial \\tilde{o}_{t+1}}, \\frac{\\partial J}{\\partial \\tilde{f}_{t+1}}, \\frac{\\partial J}{\\partial \\tilde{i}_{t+1}}, \\frac{\\partial J}{\\partial \\tilde{g}_{t+1}}$,求某个节点梯度时，首先应该找到该节点的输出节点，然后分别计算所有输出节点的梯度乘以输出节点对该节点的梯度，最后相加即可得到该节点的梯度。如计算$\\frac{\\partial J}{\\partial h_t}$时，找到$h_t$节点的所有输出节点  $ y_{t} $ 、 $ \\widehat{o}_{t+1}$、$\\widehat{f}_{t+1}$、$\\widehat{i}_{t+1}$、$ \\widehat{g}_{t+1}$ ，然后分别计算输出节点的梯度(如$\\frac{\\partial J}{\\partial y_t}$)与输出节点对 $h_t$的梯度的乘积（如 $\\frac{\\partial J}{\\partial y_t}W^T_{hg}$），最后相加即可得到节点 $h_t$ 的梯度:\n",
    "$\\frac{\\partial J}{\\partial h_{t}}=\\frac{\\partial J}{\\partial y_{t}} W_{y h}^{T}+\\frac{\\partial J}{\\partial \\tilde{o}_{t+1}} W_{h o}^{T}+\\frac{\\partial J}{\\partial \\tilde{f}_{t+1}} W_{h f}^{T}+\\frac{\\partial J}{\\partial \\tilde{i}_{t+1}} W_{h i}^{T}+\\frac{\\partial J}{\\partial \\tilde{g}_{t+1}} W_{h g}^{T} $\n",
    "\n",
    "\n",
    "\n",
    "同理可得t时刻其它节点的梯度:\n",
    "对参数的梯度：\n",
    "$\n",
    "\\left\\{\n",
    "\\begin{array}{l}\n",
    "\\frac{\\partial J}{\\partial h_{t}}=\\frac{\\partial J}{\\partial y_{t}} W_{y h}^{T}+\\frac{\\partial J}{\\partial \\tilde{o}_{t+1}} W_{h o}^{T}+\\frac{\\partial J}{\\partial \\tilde{f}_{t+1}} W_{h f}^{T}+\\frac{\\partial J}{\\partial \\tilde{i}_{t+1}} W_{h i}^{T}+\\frac{\\partial J}{\\partial \\tilde{g}_{t+1}} W_{h g}^{T} \\\\\n",
    "\\frac{\\partial J}{\\partial m_{t}}=\\frac{\\partial J}{\\partial h_{t}} \\odot o_{t} \\\\\n",
    "\\frac{\\partial J}{\\partial c_{t}}=\\frac{\\partial J}{\\partial m_{t}} \\frac{d m_{t}}{d c_{t}}+\\frac{\\partial J}{\\partial c_{t+1}} \\odot f_{t+1}+\\frac{\\partial J}{\\partial \\tilde{f}_{t+1}} W_{c f}^{T}+\\frac{\\partial J}{\\partial \\tilde{i}_{t+1}} W_{c i}^{T} \\\\\n",
    "\\left\\{\n",
    "\\begin{array}{l}\n",
    "\\frac{\\partial J}{\\partial g_{t}}=\\frac{\\partial J}{\\partial c_{t}} \\odot i_{t} \\\\\n",
    "\\frac{\\partial J}{\\partial i_{t}}=\\frac{\\partial J}{\\partial c_{t}} \\odot g_{t} \\\\\n",
    "\\frac{\\partial J}{\\partial f_{t}}=\\frac{\\partial J}{\\partial c_{t}} \\odot c_{t-1} \\\\\n",
    "\\frac{\\partial J}{\\partial o_{t}}=\\frac{\\partial J}{\\partial h_{t}} \\odot m_{t}\n",
    "\\end{array}\\right\\} \\Rightarrow\\left\\{\\begin{array}{l}\n",
    "\\frac{\\partial J}{\\partial \\tilde{g}_{t}}=\\frac{\\partial J}{\\partial g_{t}}\\left(1-g_{t}^{2}\\right) \\\\\n",
    "\\frac{\\partial J}{\\partial \\tilde{i}_{t}}=\\frac{\\partial J}{\\partial i_{t}} i_{t}\\left(1-i_{t}\\right) \\\\\n",
    "\\frac{\\partial J}{\\partial \\tilde{f}_{t}}=\\frac{\\partial J}{\\partial f_{t}} f_{t}\\left(1-f_{t}\\right) \\\\\n",
    "\\frac{\\partial J}{\\partial \\tilde{o}_{t}}=\\frac{\\partial J}{\\partial o_{t}} i_{t}\\left(1-o_{t}\\right)\n",
    "\\end{array}\\right. \\\\\n",
    "\\frac{\\partial J}{\\partial x_{t}}=\\frac{\\partial J}{\\partial \\tilde{o}_{t}} W_{x o}^{T}+\\frac{\\partial J}{\\partial \\tilde{f}_{t}} W_{x f}^{T}+\\frac{\\partial J}{\\partial \\tilde{i}_{t}} W_{x i}^{T}+\\frac{\\partial J}{\\partial \\tilde{g}_{t}} W_{x g}^{T} \\\\\n",
    "\\end{array}\n",
    "\\right.\n",
    "$\n",
    "\n",
    "对参数的梯度:\n",
    "\n",
    "$\n",
    "\\left\\{\\begin{array} { l } \n",
    "{ \\frac { \\partial J } { \\partial W _ { h o } } = h _ { t } ^ { T } \\frac { \\partial J } { \\partial \\tilde { o } _ { t + 1 } } } \\\\\n",
    "{ \\frac { \\partial J } { \\partial W _ { h f } } = h _ { t } ^ { T } \\frac { \\partial J } { \\partial \\tilde { f } _ { t + 1 } } } \\\\\n",
    "{ \\frac { \\partial J } { \\partial W _ { h i } } = h _ { t } ^ { T } \\frac { \\partial J } { \\partial \\tilde { i } _ { t + 1 } } } \\\\\n",
    "{ \\frac { \\partial J } { \\partial W _ { h g } } = h _ { t } ^ { T } \\frac { \\partial J } { \\partial \\tilde { g } _ { t + 1 } } }\n",
    "\\end{array} \\left\\{\\begin{array} { l } \n",
    "{ \\frac { \\partial J } { \\partial W _ { y h } } = h _ { t } ^ { T } \\frac { \\partial J } { \\partial y _ { t } } } \\\\\n",
    "{ \\frac { \\partial J } { \\partial W _ { c f } } = c _ { t } ^ { T } \\frac { \\partial J } { \\partial \\tilde { f } _ { t + 1 } } } \\\\\n",
    "{ \\frac { \\partial J } { \\partial W _ { c i } } = c _ { t } ^ { T } \\frac { \\partial J } { \\partial \\tilde { i } _ { t + 1 } } } \\\\\n",
    "{ \\frac { \\partial J } { \\partial W _ { c o } } = c _ { t } ^ { T } \\frac { \\partial J } { \\partial \\tilde { o } _ { t } } }\n",
    "\\end{array} \\left\\{\\begin{array}{l}\n",
    "\\frac{\\partial J}{\\partial W_{x o}}=x_{t}^{T} \\frac{\\partial J}{\\partial \\tilde{o}_{t}} \\\\\n",
    "\\frac{\\partial J}{\\partial W_{x f}}=x_{t}^{T} \\frac{\\partial J}{\\partial \\tilde{f}_{t}} \\\\\n",
    "\\frac{\\partial J}{\\partial W_{x i}}=x_{t}^{T} \\frac{\\partial J}{\\partial \\tilde{i}_{t}} \\\\\n",
    "\\frac{\\partial J}{\\partial W_{x g}}=x_{t}^{T} \\frac{\\partial J}{\\partial \\tilde{g}_{t}}\n",
    "\\end{array}\\right.\\right.\\right.\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 习题25.5\n",
    "&emsp;&emsp;推导LSTM模型中记忆元的展开式(25.26)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：**\n",
    "\n",
    "**解答思路：**  \n",
    "\n",
    "递归法,依次带入即可"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答步骤：**   \n",
    "\n",
    "已知: 当前位置的记忆元$c_t$有:\n",
    "\n",
    "式(1):\n",
    "$\n",
    "c_t = i_t\\odot\\hat{c}_t+f_t\\odot c_{t-1}$\n",
    "\n",
    "则当t=t-1时有\n",
    "\n",
    "式(2): $c_{t-1} = i_{t-1}\\odot\\hat{c}_{t-1}+f_{t-1}\\odot c_{t-2}$\n",
    "\n",
    "将式(2)带入式(1)中,有式(3):\n",
    "\n",
    "$\\\\\n",
    "c_t = i_t\\odot\\hat{c}_t+f_t\\odot(i_{t-1}\\odot\\hat{c}_{t-1}+f_{t-1}\\odot c_{t-2})\\\\\n",
    "\\ \\ \\ = i_t\\odot\\hat{c}_t+f_t\\odot i_{t-1}\\odot\\hat{c}_{t-1}+ f_t\\odot f_{t-1}\\odot c_{t-2}\n",
    "$\n",
    "\n",
    "同理,我们可以将$c_{t-2}= i_{t-2}\\odot\\hat{c}_{t-2}+f_{t-2}\\odot c_{t-3}带入上式(3)中,递推可得:\n",
    "\n",
    "$\\\\\n",
    "c_t = i_t\\odot\\hat{c}_t+f_t\\odot c_{t-1}\\\\\n",
    "\\ \\ \\ = \\sum_{i=1}^{t}(\\prod_{j=i+1}^{t}f_i \\odot i_i)\\odot \\hat{c}_i \\\\\n",
    "\\ \\ \\ = \\sum_{i=1}^{t}w^t_i \\odot \\hat{c}_{i} \\\\\n",
    "$ \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 习题25.6\n",
    "&emsp;&emsp;写出双向LSTM-CRF的模型公式。图25.16是双向LSTM-CRF的架构图。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：**\n",
    "\n",
    "**解答思路：**  \n",
    "\n",
    "根据 LSTM-CRF 架构图逐一说明即可"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答步骤：**  \n",
    "\n",
    "$\n",
    "r^{(1)}_{t}=U_{1} \\cdot h_{t-1} + W_{1} \\cdot x_{t}+b_{1} \\\\\n",
    "r^{(2)}_{t}=U_{2} \\cdot h_{t+1} + W_{2} \\cdot x_{t}+b_{2} \\\\\n",
    "h^{(1)}_{t} = tanh (r_{t}) \\\\\n",
    "h^{(2)}_{t} = tanh (r_{t}) \\\\\n",
    "z_{t} = V_1 \\cdot h^{(1)}_{t} + V_2 \\cdot h^{(2)}_{t} +c \\\\\n",
    "p_{t} = softmax(z_{t}) \n",
    "$"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c50c7698ff25eb1d7dce4c52e3d5cc14b3b23d6a97d4fb70aad8c815eba6f63e"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "273.188px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
